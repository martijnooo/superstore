---
editor_options: 
  markdown: 
    wrap: 72
---

# Superstore EDA

```{r}
# Setup
library(tidyr)
library(tidyverse)
library(ggplot2)
library(viridis)
library(dplyr)
library(lubridate)
library(psych)

superstore <- read.csv("Sample - Superstore.csv")
```

```{r}
# Exploratory analysis
str(superstore)
```

```{r}
#Checking for null values
sum(is.na(superstore))
# Checking for duplicates
sum(duplicated(superstore))

# convert columns to correct types
superstore$Order.Date <- as.Date(superstore$Order.Date, format = "%m/%d/%Y")
superstore$Ship.Date <- as.Date(superstore$Ship.Date, format = "%m/%d/%Y")

# Checking for data consistency
table(superstore$Category)
table(superstore$Region)
table(superstore$Ship.Mode)
```

```{r}
#creating new month and year column
superstore <- superstore %>%
  mutate(Year = format(`Order.Date`, "%Y")) %>%
  mutate(Month = format(`Order.Date`, "%m")) %>%
  mutate(YearMonth = format(`Order.Date`, "%Y-%m"))

superstore$Year <- as.numeric(as.character(superstore$Year))  # Keep Year numeric
```

```{r}
# handling outliers
superstore_numeric <- superstore %>%
  select(Order.ID, Sales, Profit)

superstore_numeric_long <- superstore_numeric %>%
  pivot_longer(cols = c(Sales, Profit), 
               names_to = "Variable",
               values_to = "Values")

outliers_box <- ggplot(superstore_numeric_long, aes(Values, Variable))
outliers_box + geom_boxplot()
```

```{r}
# checking outliers
sales_outlier <- superstore %>%
  subset(Sales > 5000)
```

-\> All based on high quantity, technology purchases and hence not
removed

```{r}
# checking outliers
profit_outlier <- superstore %>%
  subset(Profit > 3000 | Profit < -2500)
```

-\> Negative outliers because of large discounts and positive outliers
align with sales outliers

# Question 1: How does sales performance vary over time?

## Objective: Identify monthly trends and seasonal patterns.

### Tasks:

```{r}
# group sales by month and year
sales_by_month_year <- superstore %>%
  group_by(YearMonth) %>%
  summarize(TotalSales = sum(Sales, na.rm = TRUE))

# create a table with sales by year and month
sales_by_month_year_split <- superstore %>%
  select(Sales, Month, Year)
sales_by_month_year_split_wide <- sales_by_month_year_split %>%
  pivot_wider(names_from = Month, values_from = Sales, 
              values_fn = sum, names_prefix = "Month_")

sales_by_month_year_split_wide <- sales_by_month_year_split_wide %>%
  select(Year, sort(names(.)[-1]))

# Reshape data back to long format for plotting
sales_by_month_year_split_long <- sales_by_month_year_split_wide %>%
  pivot_longer(cols = starts_with("Month_"), names_to = "Month", 
               values_to = "Sales") %>%
  mutate(Month = as.integer(gsub("Month_", "", Month)))
```

-   Identify which months consistently show sales peaks

```{r}
# Plotting
ggplot(sales_by_month_year, aes(x = YearMonth, y = TotalSales)) +
  geom_col() +
  labs(title = "Sales Over Time", x = "Month-Year", y = "Total Sales") +
  theme(axis.text.x = element_text(angle = 90, hjust = 1))

```

-\> Shows sales peaks towards end of the year

```{r}
# normalise for better compariseon
sales_by_month_year_split_long <- sales_by_month_year_split_long %>%
  group_by(Year) %>%
  mutate(Sales_Scaled = (Sales - min(Sales)) / (max(Sales) - min(Sales))) %>%
  ungroup()
# output in heatmap
heatmap_plot <- ggplot(sales_by_month_year_split_long, aes(x = factor(Month), y = factor(Year), fill = Sales_Scaled)) 
heatmap_plot +  geom_tile() +
  scale_fill_viridis(name = "Sales", option = "C") +
  labs(x = "Month", y = "Year", title = "Sales Heatmap") +
  theme_minimal() +
  theme(axis.text.x = element_text(angle = 45, hjust = 1))
```

-\> Normalised sales per month & year confirm peaks for last 4 months of
the year with November and December dominating. Surprising small peak as
well in March.

```{r}
# multiple regression with year as continuous and month as dummy variable
sales_by_month_year_split_long$Month <- as.factor(sales_by_month_year_split_long$Month)  # Ensure Month is categorical
sales_by_month_year_split_long$Year <- sales_by_month_year_split_long$Year - min(sales_by_month_year_split_long$Year)  # Shift year so the first year is 0
model_final <- lm(Sales ~ Year + Month, data = sales_by_month_year_split_long)
summary(model_final)
```

-\> Multiple regression shows significant increase in sales per year and
confirms the significance of the large peaks at the end of the year
(September, November and December) and also shows significant deviation
in March.

```{r}
ggplot(sales_by_month_year_split_long, aes(x = Year, y = Sales)) +
  geom_point() +  # Scatter plot of sales data
  geom_smooth(method = "lm", se = TRUE, color = "blue") +  # Regression line
  labs(title = "Sales Over Years",
       x = "Years (Starting at 0)",
       y = "Sales") +
  theme_minimal()

ggplot(sales_by_month_year_split_long, aes(x = as.factor(Month), y = Sales)) +
  geom_boxplot(fill = "lightblue") +
  labs(title = "Monthly Sales Seasonality",
       x = "Month",
       y = "Sales") +
  theme_minimal()
```

-   Calculate YOY growth rate for the most recent complete year

```{r}
# grouping by year
sales_by_year <- superstore %>%
  select(Sales, Year) %>%
  group_by(Year) %>%
  summarise(TotalSales =sum(Sales))

#calculating growth
sales_by_year <- sales_by_year %>%
  mutate(YoY_Growth = round((TotalSales / lag(TotalSales) -1)*100,2)) %>%
  subset(Year > 2014)

# plotting
growth_plot <- ggplot(sales_by_year, aes(x=factor(Year), y=YoY_Growth, fill=YoY_Growth > 0)) 
growth_plot+
  geom_col() +
  scale_fill_manual(values=c("FALSE"="red", "TRUE"="darkgreen")) +
  labs(title = "YoY Growth Rate", y="Growth in %", x="Year") +
  geom_text(aes(label=sprintf("%.1f", YoY_Growth))) +
  theme_minimal() + theme(legend.position = "none")
```

-   Suggest potential reasons for seasonal patterns

    --\> Potential reasons include christmas for November and December
    and end of season sales in winter (March) and summer (September)

```         
```

# Question 2: Which product categories have the best/worst profit margins?

## Objective: Identify high/low performing product categories

### Tasks:

-   Identify which category has the thinnest margins

```{r}
# create df for categories
profit_margin_by_cat <- superstore %>%
  select(Category, Sales, Profit) %>%
  group_by(Category) %>%
  summarise(TotalSales = sum(Sales), TotalProfit =sum(Profit)) %>%
  mutate(ProfitMargin = round(TotalProfit/TotalSales*100,2)) %>%
  arrange(ProfitMargin)

cat_lowest_margin = slice(profit_margin_by_cat,1)
cat_lowest_margin
```

-\> The Furniture Category has the lowest Margins with 2.49%

-   Calculate the profit margin difference between top and bottom
    categories

```{r}
highest_margin = profit_margin_by_cat[nrow(profit_margin_by_cat),]$ProfitMargin
highest_margin - cat_lowest_margin$ProfitMargin
```

-\> The difference is 14.91% between the Furniture(lowest) and
Technology (highest) Category Profit Margin

-   Suggest strategies to improve low-performing categories

-\> Check for subcategories within the category to see if there are
products that can be dropped, check price increases or cost reduction
measurements. At the same time, the sales volume for the category must
be checked to make any early conclusions. It could be that the category
has much higher sales rates and naturally lower profit margins but still
resulting in high overall Profit.

# Question 3: How do regional performances compare?

## Objective: Compare sales distribution and profitability across regions

### Tasks:

-   Identify which region has both high sales and high profitability

```{r}
# create df for categories
performance_by_region <- superstore %>%
  select(Region, Sales, Profit) %>%
  group_by(Region) %>%
  summarise(TotalSales = sum(Sales), TotalProfit =sum(Profit)) %>%
  mutate(ProfitNormalised = TotalProfit / sum(TotalProfit),
         SalesNormalised = TotalSales / sum(TotalSales), 
         Performance = ProfitNormalised + SalesNormalised) %>%
  mutate(ProfitMargin = round(TotalProfit/TotalSales*100,2)) %>%
  arrange(desc(Performance))

performance_by_region[1,]$Region

```

The Region "West" shows the highest Sales and Profitability

-   Find any regions with negative profits

```{r}
regions_with_loss <- performance_by_region %>%
  subset(TotalProfit < 0)
regions_with_loss

```

-\> There are no regions with negative profits

-   Analyze if high sales always correlate with high profits

```{r}
sale_profit_plot <- ggplot(performance_by_region, aes(x=TotalSales, y=TotalProfit))
sale_profit_plot + geom_point() + geom_smooth(method = "lm")
```

```{r}
# Fit the linear regression model
model <- lm(TotalProfit ~ TotalSales, data = performance_by_region)

# Display the summary of the model
summary(model)
```

-\> Correlation of 0.85 shows strong relation between Sales and Profit.
However, with a p-value of 7%, this is statistically insignificant

-   Propose regional-specific strategies based on findings

-\> Profit Margin of the Central Region seems to be the only clear
outlier and needs to be further evaluated. Regional performance needs to
be contrasted to factors such as market size and market share in the
various regions to derive appropriate conclusions on the differing
values in the analysis.

# Question 4: What does customer segmentation reveal?

## Objective: Identify valuable customer groups using RFM analysis

### Tasks:

-   Calculate percentage of customers in each segment
-   Identify which segment generates the most revenue

```{r}
customer_segment <- superstore %>%
  select(Customer.ID, Segment, Sales) %>%
  group_by(Segment) %>%
  summarise(Customers = n_distinct(Customer.ID), TotalSales = sum(Sales)) %>%
  mutate(CustomersShare = round(Customers/ sum(Customers)*100,2))
print(customer_segment)
```

-\> Consumer with most customers (50%) and 50% more Sales than second
category (corporate) with about \$1.2m

-   Develop retention strategies for “At Risk” customers

```{r}
# Get relevant customer data
at_risk <- superstore %>%
  select(Order.Date, Customer.ID, Sales) %>%
  group_by(Customer.ID) %>%
  summarise(
    LastPurchase = max(Order.Date),
    Spending = sum(Sales),
    TotalPurchases = n(),
    NumberPurchasesLast3Months = sum(Order.Date >= as.Date("2018-01-01") - months(3)),
    NumberPurchases3_6Months = sum(Order.Date >= as.Date("2017-10-01") - months(3)),
    PurchaseHistoryDiff = NumberPurchasesLast3Months - NumberPurchases3_6Months,
    SalesLast3Months = sum(Sales[Order.Date >= as.Date("2018-01-01") - months(3)]),
    Sales3_6Months = sum(Sales[Order.Date >= as.Date("2017-10-01") - months(3)]),
    SalesHistoryDiff = SalesLast3Months - Sales3_6Months
  )

# Add recency and scores
at_risk <- at_risk %>%
  mutate(
    Recency = as.numeric(as.Date("2018-01-01") - LastPurchase),
    RecencyScore = case_when(
      Recency <= 30 ~ 3,
      Recency <= 60 ~ 2,
      Recency <= 90 ~ 1,
      Recency > 90 ~ 0
    ),
    FrequencyScore = case_when(
      NumberPurchasesLast3Months != 0 & (NumberPurchasesLast3Months - PurchaseHistoryDiff > 2) ~ 3,
      NumberPurchasesLast3Months != 0 & (NumberPurchasesLast3Months - PurchaseHistoryDiff > 0) ~ 2,
      NumberPurchasesLast3Months != 0 & (NumberPurchasesLast3Months - PurchaseHistoryDiff > -2) ~ 1,
      TRUE ~ 0
    ),
    SalesScore = case_when(
      SalesLast3Months != 0 & (SalesLast3Months - SalesHistoryDiff > 20) ~ 3,
      SalesLast3Months != 0 & (SalesLast3Months - SalesHistoryDiff > 0) ~ 2,
      SalesLast3Months != 0 & (SalesLast3Months - SalesHistoryDiff > -20) ~ 1,
      TRUE ~ 0
    )
  )

```

```{r}
at_risk <- at_risk %>%
  mutate(RiskScore = RecencyScore + FrequencyScore + SalesScore) %>%
  mutate(RiskScore = ifelse(Recency > 365, NA,RiskScore )) %>%
  mutate(CustomerStatus = case_when(
    RiskScore == 0 ~ "Very High Risk",
    RiskScore > 0 & RiskScore <7 ~ "Medium Risk",
    RiskScore >= 7 ~ "Low Risk",
    TRUE ~ "Lost"
  ))
```

```{r}
ggplot(at_risk, aes(x = factor(CustomerStatus, levels = c("Lost", "Very High Risk", "Medium Risk", "Low Risk")))) +
  geom_bar() + theme_minimal() + labs(title = "Customer Count by Risk Category", x="Risk Category", y= "Count")
```

-\> There are about 100 customers defined as lost, which have not done a
purchase within the past year. While reactivation is unlikely, these
customers could be sent a discount code for their next purchase. Very
high risk customers have a combination of low purchase recency,
decreasing purchase frequency trend or decreasing monetary spending
amount trend. As these customers have conducted a purchase in recent
time though, they can be offered complementary products to their recent
purchases.

-   Suggest marketing approaches for “High Spenders”

```{r}
# extracting high spenders
high_spenders <- superstore %>%
  select(Customer.ID, Sales, Segment) %>%
  group_by(Customer.ID) %>%
  summarise(TotalSales = sum(Sales),
            Segment = last(Segment)) %>%
  arrange(desc(TotalSales))

```

```{r}
high_spender_plot <- ggplot(high_spenders, aes(x=Segment, y=TotalSales))
high_spender_plot + geom_boxplot()
```

```{r}
high_spenders_top <- high_spenders %>%
  subset(TotalSales > 7500)
length((high_spenders_top$TotalSales))
```

```{r}
high_spender_plot2 <- ggplot(high_spenders_top, aes(x=TotalSales, fill=Segment))
high_spender_plot2 + geom_histogram(binwidth = 2500, color="black") + theme_minimal() + labs(title="Customers with spent > $7500", y="Count", x="Sales Amount")
```

--\> There are about 50 customers with a spending of more than \$7500
(about 7% of customers). These are predominantly in the consumer,
followed by the corporate segment. It is recommended that an account
executive is hired, working closely together with these kind of
customers to exactly understand their needs and address them (e.g.
discounts for more orders, complementary products to what they already
need).

# Question 5: How does shipping mode affect profitability?

## Objective: Analyze cost-to-serve across shipping modes

### Tasks:

-   Compare profit margins across shipping modes

```{r}
shipping_grouped <- superstore %>%
  select(Ship.Mode, Sales, Profit) %>%
  group_by(Ship.Mode) %>%
  summarise(TotalSales = sum(Sales), 
            TotalProfit = sum(Profit),
            Count = n()) %>%
  mutate(ProfitMargin = round(TotalProfit / TotalSales*100,2), 
         ProfitPerOrder = round(TotalProfit / Count,2)) %>%
  arrange(desc(ProfitMargin))

```

```{r}
shipping_plot <- ggplot(shipping_grouped, aes(x=Ship.Mode, y=ProfitMargin))
shipping_plot + geom_col() + theme_minimal() + labs(titel="Profit Margin by Shipping Mode", x="Shipping Mode", y="Profit Margin in %")
```

-\> First Class has the highest Profit Margin, however only by a small
margin to the others.

-   Calculate profit per order for each shipping mode

```{r}
profit_per_order_plot <- ggplot(shipping_grouped, aes(x=Ship.Mode, y=ProfitPerOrder))
profit_per_order_plot + geom_col() + theme_minimal() + labs(titel="Profit per Order by Shipping Mode", x="Shipping Mode", y="Avg. Profit per Order")
```

```{r}
ggplot(superstore, aes(x=Ship.Mode, y=Profit)) + geom_boxplot() + theme_minimal() + labs(title = "Distribution Profit per Order per Shipping Mode", x="Shipping Mode", y="Profit per Order")
```

```{r}
anova_result <- aov(Profit ~ Ship.Mode, data = superstore)
summary(anova_result)
```

-   Suggest optimal shipping strategy based on findings

-\> The Anova shows no significant difference in the profit per order
between the shipping types. Hence, there is no need to focus on shipping
type to increase profitability of the company.

## Extra Challenge:

-   Identify 3 actionable business recommendations.

1.  Develop a customer retention strategy, as this is typically cheaper
    than acquiring new customers
2.  Evaluate discontinuation of furniture category due to low profit
    margins
3.  Analysis of discount strategy which frequently resulted in loss
    making orders

-   Propose 2 new questions for deeper analysis.

## Can we identify customer clusters based on their spending behaviour?

```{r}
# get the purchase frequency and sales amount
customer_segmentation <- superstore %>%
  select(Customer.ID, Sales) %>%
  group_by(Customer.ID) %>%
  summarise(Purchases = n(),
            TotalSales = round(sum(Sales)))
```

```{r}
# Scale the data (important for k-means)
customer_scaled <- customer_segmentation %>%
  select(Purchases, TotalSales) %>%
  scale()  # Standardize the variables
```

```{r}
# Elbow method
wss <- numeric(10)  # To store WSS for different cluster sizes
for (k in 1:10) {
  kmeans_result <- kmeans(customer_scaled, centers = k)
  wss[k] <- kmeans_result$tot.withinss
}

# Plotting the elbow plot
elbow_plot <- ggplot(data.frame(k = 1:10, wss = wss), aes(x = k, y = wss)) +
  geom_line() +
  geom_point() +
  labs(title = "Elbow Method for Optimal Number of Clusters",
       x = "Number of Clusters",
       y = "Within-cluster Sum of Squares (WSS)")

print(elbow_plot)
```

```{r}
library(cluster)

sil_score <- numeric(10)
for (k in 2:10) {  # Silhouette score isn't meaningful for k = 1
  kmeans_result <- kmeans(customer_scaled, centers = k)
  sil_score[k] <- mean(silhouette(kmeans_result$cluster, dist(customer_scaled))[, 3])
}

# Plot silhouette scores
silhouette_plot <- ggplot(data.frame(k = 2:10, sil_score = sil_score[2:10]), aes(x = k, y = sil_score)) +
  geom_line() +
  geom_point() +
  labs(title = "Silhouette Method for Optimal Number of Clusters",
       x = "Number of Clusters",
       y = "Silhouette Score")

print(silhouette_plot)

```

```{r}
# Apply k-means clustering
set.seed(42)  # For reproducibility
kmeans_result <- kmeans(customer_scaled, centers =3)

# Add cluster labels to your data
customer_segmentation$Cluster <- as.factor(kmeans_result$cluster)

# Visualize the clusters
ggplot(customer_segmentation, aes(x=Purchases, y=TotalSales, color=Cluster)) +
  geom_point() +
  labs(title = "Customer Segmentation by Purchases and Total Sales")
```

```{r}
customer_segmentation %>%
  group_by(Cluster) %>%
  summarise(Count = n(), Sale = sum(TotalSales)) %>%
  mutate(AvgSale = round(Sale/Count,2),
         CountShare = round(Count/ sum(Count)*100,2),
         SaleShare = round(Sale / sum(Sale)*100,2))
```

-\> We identified 3 customer clusters based on spending behaviour. About
50% of customers have a low number of purchaes with a low spending
amount. It should be checked if there spending behaviour can be further
enforced, as these are the upcoming / promising customers. about 40% of
customers have a high amount of purchases and medium amout of Sales. 10%
of customers generate about 27% of sales and hence represent the most
important customer group for the business.

## Can we identify the most frequent product bundles purchased together, and what impact would targeted cross-selling have on sales and profit?

```{r}
library(arules)
```

```{r}
product_bundles <- superstore %>%
  select(Order.ID, Sub.Category) #Category instead

write.csv(product_bundles, "filename.csv", row.names = FALSE)
```

```{r}
# Convert the data frame to a transaction format
transactions <- as(split(product_bundles$Sub.Category, product_bundles$Order.ID), "transactions")
# Apply the Apriori algorithm to find frequent itemsets and generate rules
rules <- apriori(transactions, parameter = list(supp = 0.002, conf = 0.5))

# View the top rules based on lift
inspect(sort(rules, by = "lift"))
```

-\> The analysis of the `product_bundles` dataset has revealed that
**Binders** frequently appear as a consequential product in association
rules. This indicates that **Binders** are often purchased alongside
other items, such as **Appliances, Furnishings, Storage, and Paper**.
The frequent presence of **Binders** on the right-hand side (rhs) of the
rules suggests that when customers purchase combinations of these other
products, they tend to also buy **Binders** with a relatively high
likelihood.

### Crosselling Potential of Binders (1 Example)

```{r}

AvgSalesBinders = median(subset(superstore, Sub.Category == "Binders")$Sales)
AvgSalesBinders

```

-\> Binders are purchased for a value of about \~\$20, when purchased

```{r}
binder_potential <- superstore %>%
  select(Sub.Category, Order.ID, Order.Date) %>%
  #filter(Order.Date > as.Date("2017-01-01")) %>%
  group_by(Order.ID) %>%
  summarise(Category = list(Sub.Category))
```

```{r}
# combination that sets up for Binder purchase
sum(sapply(binder_potential$Category, function(x) all(c("Appliances", "Storage") %in% x)))
# already bought binder
sum(sapply(binder_potential$Category, function(x) all(c("Appliances", "Storage", "Binders") %in% x)))
```

-\> With targeted cross-selling there was an additional sales potential
of about \$500 ((56-30) \* 20) in the past 4 years.

## Discussion Points:

-   How do sales trends correlate with marketing initiatives?

    -\> no input on marketing available

-   Are there regional preferences for product categories?

```{r}
product_by_region <- superstore %>%
  select(Category, Region)

contingency_table <- product_by_region %>%
  table()
# Perform the chi-square test
chi_square_result <- chisq.test(contingency_table)

# View the result
chi_square_result
```

-\> Chi Square test shows no significant preference for product
categories in regions.

-   What operational changes could improve low-margin categories?

    -\> Reduce various costs related to product (purchase price,
    overheads)

-   How might customer segmentation affect inventory management?

    -\> Segmenting customers by region and product category allows to
    understand what kind of products are bought in certain regions
